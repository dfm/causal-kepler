% This file is part of the causal-kepler project
% Copyright 2013 the authors.

\documentclass[12pt, preprint]{aastex}

\newcommand{\notenglish}[1]{\textit{#1}}
\newcommand{\sic}{\notenglish{sic}}
\newcommand{\project}[1]{\textsl{#1}}
\newcommand{\Kepler}{\project{Kepler}}
\newcommand{\name}{PLM}
\newcommand{\todo}[1]{\textbf{#1}}

\begin{document}

\title{%
  A data-driven, pixel-level model to improve the precision of \Kepler\ photometry%
}
\author{%
  Dan~Foreman-Mackey\altaffilmark{\ref{CCPP}},
  David~W.~Hogg\altaffilmark{\ref{CCPP},\ref{MPIA},\ref{email}},
  Tom~Barclay\altaffilmark{\ref{Ames}},
  Rob~Fergus\altaffilmark{\ref{Courant}},
  Stefan Harmeling\altaffilmark{\ref{MPIIS}},
  Bernhard~Sch\"olkopf\altaffilmark{\ref{MPIIS}},
  others%
}

\newcounter{address}
\setcounter{address}{1}
\altaffiltext{\theaddress}{\stepcounter{address}\label{CCPP}%
  Center for Cosmology and Particle Physics, Department of Physics, New York University}
\altaffiltext{\theaddress}{\stepcounter{address}\label{MPIA}%
  Max-Planck-Institut f\"ur Astronomie, Heidelberg, Germany}
\altaffiltext{\theaddress}{\stepcounter{address}\label{email}%
  To whom correspondence should be addressed; \texttt{<david.hogg@nyu.edu>}.}
\altaffiltext{\theaddress}{\stepcounter{address}\label{Ames}%
  NASA Ames Research Center}
\altaffiltext{\theaddress}{\stepcounter{address}\label{Courant}%
  Courant Institute of Mathematical Sciences, New York University}
\altaffiltext{\theaddress}{\stepcounter{address}\label{MPIIS}%
  Max-Planck-Institut f\"ur Intelligente Systeme, T\"ubingen}
%% \altaffiltext{\theaddress}{\stepcounter{address}\label{Oxford}%
%%   Department of Physics, Oxford University}
%% \altaffiltext{\theaddress}{\stepcounter{address}\label{CfA}%
%%   Harvard--Smithsonian Center for Astrophysics}
%% \altaffiltext{\theaddress}{\stepcounter{address}\label{UCL}%
%%   Department of Physics and Astronomy, University College London}
%% \altaffiltext{\theaddress}{\stepcounter{address}\label{CMU}%
%%   McWilliams Center for Cosmology, Carnegie Mellon University}
%% \altaffiltext{\theaddress}{\stepcounter{address}\label{Caltech}%
%%   Department of Astronomy, California Institute of Technology}
%% \altaffiltext{\theaddress}{\stepcounter{address}\label{Columbia}%
%%   Department of Astronomy, Columbia University}

\begin{abstract}
The precision of \Kepler\ photometry%
  ---the most precise photometric measurements of stars ever made---%
  appears to be limited by unknown or untracked variations
  in spacecraft pointing, point-spread function, stellar motions and parallaxes, and temperature.
Here we present \name,
  a data-driven model of these variations in \Kepler\ data (or really of their impact on the photometry).
Importantly (and uniquely),
  \name\ works at the pixel level (not the photometric measurement level);
  it can capture more fine-grained information about the variation of the spacecraft
  (especially regarding pointing and point-spread function)
  than is available in the pixel-summed photometry.
\name\ is extreme in that it has enormous flexibility and it only uses data (and meta-data) to model data:
\name\ provides a prediction for each readout pixel built from a linear combination of the readouts
  from very large numbers of nearby pixels (plus some spacecraft information);
  the choice of nearby pixels is guided by ideas from causal inference.
\name\ avoids over-fitting by employing a train-and-test formalism
  designed to ensure that transit-like photometric events and short-timescale stellar variability
  cannot be captured or distorted by the model;
  it is designed to remove spacecraft-induced variability but not intrinsic stellar variability.
We show that \name\ outperforms the \Kepler\ Presearch (\sic) Data Conditioning method on a set of example stars.
We release open-source code that provides \name\ output for any star in the existing \Kepler\ Archive,
  and we discuss applications for other missions,
  including any possible \Kepler\ two-wheel mission.
\end{abstract}

\section{Ultra-precise photometry}

The photometric measurements of stars made by the \Kepler\ Satellite are precise enough
  to permit discovery of exoplanet transits with depths smaller than $10^{-4}$.
This precision results from great spacecraft stability,
  supplemented by various methods for removing small residual spacecraft-induced and stellar-variability trends in the brightnesses,
  either filtering the data (with things like median filters; CITE)
  or fitting the data with flexible models (like polynomials or splines or Gaussian Processes; CITE).
When employed in the service of exoplanet search and characterization,
  these methods are usually agnostic about whether photometric variations originate in the spacecraft or in the star itself;
  that is, they obliterate intrinsic stellar variability along with spacecraft issues.

In general, there are many reasons for apparent photometric variability in a \Kepler\ source.
There is intrinsic stellar variability,
  which is of interest to some and a nuisance to others.
There is also variability of overlapping fainter stars;
  that is, confusion noise combined with variability of the confusing sources.
There are small changes in spacecraft pointing,
  which leads to slightly different illumination of the focal-plane pixels,
  and thus different sensitivity to errors (problems) in the device flat-field or sensitivity map.
There are also \emph{intra-pixel} sensitivity variations that can contribute (CITE WHITEPAPER, SPITZER).
There are small changes in spacecraft temperature,
  which lead to point-spread function (PSF) and differential (across the focal plane) pointing changes.
These also lead to changes in pixel and intra-pixel illumination.
Stellar proper motion, geometric parallaxes, and differential stellar aberration as the spacecraft orbits all do more of the same.
There is electronic cross-talk between detectors and charge-transfer inefficiency;
  these can effectively transfer variability from one source to another.
There are additional electronics effects like ``rolling bands'' that put additional features into lightcurves.
There are also probably changes to the detector sensitivity with temperature and time,
  possibly illimunation-history effects,
  and possibly sources of variability not yet considered.
The remarkable thing about \Kepler\ is that it is trying to measure stars at a level of precision
  much higher than ever previously attempted;
  new effects really \emph{must} appear at some point.
In \figurename~\ref{fig:pixelpatch}, we show the pixel-level variability in the \Kepler\ data
  near one bright star that shows minimal intrinsic variability;
  this figure highlights the spacecraft-induced effects.

We propose and advise dealing with these variations in \Kepler\ light-curves by \emph{modeling} them.
In this context, a model is (at least) a parameterized function that can predict data, given parameter settings,
  and an objective function that can be optimized or sampled to choose or set those parameters.
Ideally the objective function is probabilistic, or has a probabilistic justification,
  as with a likelihood or a posterior pdf.
The model can be a physical model (of the spacecraft PSF, pointing, temperature, and so on)
  or it can be a flexible, effective model that has no direct interpretation in terms of physical spacecraft parameters.
In principle a physical model will do a better job,
  because it necessarily embodies more prior information,
  but it requires research and intuition about dominant effects,
  and this research and intuition is often wrong or incomplete.
The model we propose here is in the non-physical, effective category.
The \Kepler\ community is familiar with these kinds of models;
  to our knowledge, \emph{all} successful light-curve ``de-trending'' methods
  are flexible, effective models.

One such method---one that is designed to model spacecraft-induced problems
  but \emph{not} interfere with measurements of stellar variability---%
  is the \Kepler\ Presearch (\sic) Data Conditioning (PDC) method (CITE).
The PDC ``de-trends'' the \Kepler\ lightcurves by fitting them with a small set of basis lightcurves
  generated from a principal components analysis (PCA) of filtered lightcurves.
That is, it uses data to model data,
  regularizing the fit (and avoiding over-fitting) by filtering and restricting the dimensionality (through PCA).
The method proposed here, \name, is very similar in spirit to the PDC.
The main differences are
  (1)~that \name\ works in the pixel domain, not the lightcurve domain, so it has access to more fine-grained information,
  (2)~that \name\ has far more freedom (far more parameters) than the PDC
  but it strictly avoids over-fitting the lightcurves on exoplanet-transit time-scales
  through strong regularization and a train-and-test framework, and
  (3)~that when \name\ fits a star, there is no possibility of any contribution of the star itself to the fit basis vectors.
The two methods (\name\ and PDC) are similar however,
  in that they both make the assumption that whatever spacecraft effects are imprinting variability on a stellar lightcurve
  must also be imprinting similar or related variability on other lightcurves.
They also assume that the relationships can be captured with linear models.
By going to the pixel level (unlike the PDC, which works at the photometry level),
  \name\ makes it easier for the model to capture variability
  that is coming through variations in the centroids and point-spread function
  from spacecraft pointing, roll, and temperature.

Before we start, a few reminders about the \Kepler\ data are in order:
The satellite always observes precisely the same field, at fixed pointing (as closely as possible).
The satellite is rolled by 90~deg every 90-ish days (for Solar Angle constraints).
The PSF varies strongly across the field and is badly sampled.
The stars span a huge range in brightness;
  some of \Kepler's most important targets even saturate the device and bleed charge.
The stellar photometry returned by the \Kepler\ SAP and PDC pipelines is based on
  straight, \emph{unweighted} sums of pixels in small patches centered on the stellar centroid.
We will return to this latter point at the end;
  this kind of photometry cannot be optimal;
  it must be possible to do a better job with the photometric measurements.
That's beyond the scope of this paper but a place for a valuable intervention on the \Kepler\ data.

The scope of this paper is an intervention---\name---that makes the \Kepler\ photometry more precise.
We describe the method and deliver all the relevant code in a public, open-source repository.
We also provide an interface to the \Kepler\ data that delivers improved ``\name\ photometry''
  for every \Kepler\ target.

\section{Data-driven models}

The first general idea about data-driven modeling of the kind used here
  is that each data point or data source is going to be modeled
  (or really \emph{predicted})
  with a parameterized mathematical function of \emph{other data}.
That is, given a choice of some \emph{target} \Kepler\ data,
  we are going to find parameters of a function that takes as input some of the other \Kepler\ data,
  and provides as output predictions of the target data.
The simplest models are \emph{linear models},
  in which predictions for the target data are built from linear combinations of the other data,
  and in which the objective functions are quadratic in the prediction residual.
Examples of quadratic objective functions include Gaussian likelihoods, mean-squared-error, and chi-square statistics.
These models are simple,
  not just because they are easy to express and compute,
  but because optimization is convex:
There is only one optimum for the objective function.

The point of these models is to be \emph{flexible},
  so the usual approach is to make the input data set very large,
  and the number of parameters large,
  often as large as---or larger than---the target data available.
Such fits require \emph{regularization} to break degeneracies
  and control ill-constrained parameters.
These regularizations do for optimizations what priors do for posterior probability inferences;
  they express the desired behavior of the fit in the absence of data
  or along directions in parameter space in which the data are not constraining.
General regularizations or priors will break the convexity of linear models;
  if convexity is to be maintained, these regularizations also have to be quadratic,
  or else one of a small number of other known forms (one of which is L1-regularization).
Given these considerations, it makes sense to try to build our pixel-level model
  using a linear model with a quadratic objective function and a quadratic regularization;
  this is what \name\ will be.

[General formula here for a pixel predicted using a linear combination of other pixels.
  And the objective function, generally?]

The second general idea about data-driven modeling is the investigator's beliefs
  about the causal processes that generate the data
  are crucial in restricting the kinds of data that can be used as input to the model.
That is, different assumptions about the physical properties of the data
  and the data-taking system
  lead to different structures for the data-driven model.
For example, in the case of \Kepler\ data,
  if we believe that different stars in the \Kepler\ field vary independently
  (that is, are not physically synchronized in any way),
  then the only reason that one star might show variability that is strongly \emph{predictive}
  (useful for prediction) of another star's variability
  is that both stars are being observed by the same device or spacecraft.
That is, one star's pixels can be used to predict another star's pixels
  inasmuch as spacecraft issues imprint on both stars in related ways;
  they share a common cause.
For another example, if we think the spacecraft is being affected by
  processes that take place over time-scales longer than a single read-out
  (for example, thermal processes),
  then it would be sensible to model the data at time $t$ using not just simultaneous data,
  but also data from a range of times around $t$.
For another example, if an investigator doesn't care about preserving stellar variability,
  and just wants to detect exoplanet transits (say),
  pixels from the target star can be used to predict pixels from the target star,
  provided they are at large-enough time lags that they don't overlap (in time)
  the signals of interest.
That is, if the model is designed to fit not just spacecraft variability
  but also intrinsic stellar variability,
  the predictive model will be permitted to use as input pixels that \emph{do} overlap the target star.
In what follows, we are only going to use input data that do not overlap that target star pixels.
These models ought to remove spacecraft variability but not intrinsic stellar variability.

The third general idea is that we need to control for over-fitting.
That is, once you have a flexible-enough model you can in principle fit \emph{anything},
  whether it was caused by the spacecraft, intrinsic stellar variability, or a transiting exoplanet.
How do you prevent a very flexible model from taking small noise-induced fluctuations in all the input data
  and carefully combining them linearly into detailed models for every nook and cranny in the target data?
In most projects in astrophysics, over-fitting is controlled for by limiting model freedom.
The model is restricted in the number of parameters
  (as in ``you can't have more parameters than data points'')
  or by limiting the dimensionality
  (as in principal components analysis)
  or by applying strong priors
  (as with smoothness priors, that effectively reduce freedom without explicitly reducing the number of parameters).
In each case, the restriction on the model is controlled by some \emph{hyper-parameter}
  (such as the number of inputs, or number of principal components, or the strength of the smoothness prior).
The hyper-parameters can be set or tested with tools like cross-validation,
  the fully marginalized likelihood (Bayes factor or evidence),
  chi-squared statistical criteria,
  or intuition or heuristics.
Here we take a different and more general approach, which is to use a train-and-test framework.

In this framework, the data used to \emph{train} the model%
  ---meaning set the values of the parameters of the model---%
  are disjoint from the \emph{test} data
  ---meaning the data that are being predicted (the target data).
Because we are extremely concerned with detecting Earth-analog exoplanet transits,
  which take about ten hours,
  we adopt an extreme version of the train-and-test framework,
  in which the training data are always separated from the test data by at least ten hours.
That is, when we are using the model to predict a particular pixel in the target data taken at time $t$,
  we use parameters obtained by an optimization that makes use of only data
  that comes either at times prior to $t-\Delta t$ or after $t+\Delta t$,
  where $\Delta t$ is a tunable parameter but which we will set to roughly $10$~h.
This ensures that no information about any exoplanet transit itself
  can be entering into the prediction of the pixels contributing to the stellar photometry.
In general, if there is a scientific goal of preserving intrinsic stellar variability,
  or transit signals,
  on time-scales of $\tau$,
  the parameter optimization ought to be based on training data taken with a time-exclusion zone of half-width $\Delta t > \tau$.

% need to choose consistent notation or terminology about the contiguous data chunks.

In addition, each training data set within which we set the parameters (by optimization) has a finite total time extent $t_{\max}$.
The train-and-test framework, which includes data out to time $t_{\max}$ but excludes data within $\Delta t$ of the test data,
  effectively assumes that the signals worth preserving have time-scales less than $\Delta t$ but don't recur on time-scales
  shorter than $t_{\max}$.
Those assumptions are good for our purposes but not necessarily ideal for all users:
Short-period exoplanets and certain kinds of stellar variability
  could be wiped out by a model with these settings of the training-data regime.

The fourth general idea involved in this kind of data-driven modeling is that the models aren't \emph{interpretable}.
This means, in particular, that although the model might do a good job of \emph{predicting} the target pixels,
  using a linear (or more complex) combination of the input pixel data,
  it won't deliver anything that can be unambiguously interpreted as the \emph{flux} of the star in question,
  or any other signal we care about.
The data-driven model \emph{effectively} describes the pointing, point-spread function, and flat-field
  of the telescope and camera,
  plus the variability of every star and every exoplanet transit,
  but it does so without ever \emph{explicitly} creating any of those objects.
We have to make some kind of heuristic or interpretive move to extract from the data-driven model the quanitites of interest.

In the context of this project, the quantities of interest are the light-curves
  (photometry as a function of time or apparent flux as a function of time)
  for every star.
What we get, for every star, from the data-driven model, is a train-and-test prediction
  for the brightness of every pixel in the patch of the detector relevant to the star,
  along with the residual, or the difference between the measured read-out pixel value and the prediction (data minus model).
What will be called the ``\name\ photometry'' in what follows is constructed from what will be called the ``\name\ images''.
The \name\ photometry is constructed from the \name\ images
  just as the \Kepler\ official SAP photometry is constructed from the \todo{XXX images};
  that is, by summing pre-defined sets of pixels (with unit weights; see above and below for criticism of this).
The \name\ images are constructed from the data-driven model predictions by taking,
  for each pixel in each contiguous data segment (see below),
  the \emph{mean} predicted pixel value (mean over time),
  and adding to it the residuals (data minus model) away from the data-driven model prediction.
These \name\ images are ``calibrated'' images,
  in that the data-driven model has been used to fit out all the variability it can,
  subject to the restrictions imposed by its finite freedom and the train-and-test specification.
The signal---in which exoplanet transits will be found---is fundamentally the observed residual away from this prediction,
  but it has to be added back into the mean prediction to have the right ``baseline'' or zero level.
In short, \name\ will produce prediction images, residual images, \name\ images (mean prediction plus residual),
  and aperture-based \name\ photometry analogous to the SAP photometry.

Finally, the fifth general idea is that there is no objective \emph{ground truth} against which we can tell
  that any particular data-driven model is better or worse than any other.
This problem is a problem for \name, and for the \Kepler\ PDC, and any other data-driven calibration models.
One might think that the ``best'' model is the one that predicts data with lowest variance;
  this would be true if all models adhered to the same train-and-test framework, which they don't.
Besides, a model that can predict not just the spacecraft-induced variability,
  but also variability caused by exoplanet transits of interest,
  will effectively over-fit and distort the most important information in the light-curves.
That is, what is considered best for modeling the data depends on the objectives of the user.
For us, who are interested (in the long term) in finding and characterizing Earth analogs,
  the ``best'' data-driven model is the one that produces the most success in finding and characterizing Earth analogs!
What we will show in what follows is that the \name\ photometry does not distort
  artificial exoplanet transit signals injected into real \Kepler\ pixel-level data.
We can also show that it produces stellar light-curves that look sensible for known variable stars.
We cannot yet show that it is optimal for our long-term goals,
  nor can we show that it produces images or light-curves that are more accurately calibrated;
  we will only show promising properties of the \name\ outputs.
In the end, the value of the \name\ will be demonstrated by the scientific projects it enables.

\section{\name\ specification and hyper-parameters}



\section{Examples and results}

\section{Discussion}

Return to the SAP photometry ``unweighted sum'' issue here.

Didn't do anything about short-cadence data; suck it up.

Lots of quantitative and qualitative choices that ought to be explored.

Not the least of these is linearity; deep learning and so on.

The weakest point is construction of photometry from the model plus residuals;
  interesting whether the output of the model can be interrogated
  to provide value for those building physical models.

\acknowledgements
It is a pleasure to thank the whole \Kepler\ Team
  for designing, delivering, and operating a great facility,
  and for making all of the data public, in all its rawest forms, through the MAST interface.
We are also pleased to thank
  Ruth~Angus (Oxford),
  Bekki~Dawson (Berkeley),
  Michael~Hirsch (UCL),
  Dustin~Lang (CMU),
  Benjamin~T.~Montet (Caltech),
  and
  David~Schiminovich (Columbia)
for valuable discussions, input, encouragement, and advice.

\todo{White-paper Authors:  Feel free to contribute to this paper and move from the ack list to the author list.}

\clearpage
\begin{figure}
~ % DFM: replace this ~ with a figure
\caption{A patch of \Kepler\ imaging from \todo{XXX quarter}, centered on \todo{YYY target}.
  In the top panel, each box represents a pixel.
  Within each box is shown the light-curve of that pixel over the quarter.
  In the bottom panel, the SAP photometry for this target is shown;
  the SAP photometry is based on a straight sum of the pixels marked with darker borders.
  Note that every pixel varies substantially but the total SAP photometry is better than any individual pixel.
  Note also that the pixel variations are strongly inter-related.\label{fig:pixelpatch}}
\end{figure}

\end{document}
