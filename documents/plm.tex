% This file is part of the causal-kepler project
% Copyright 2013 the authors.

\documentclass[12pt, preprint]{aastex}

\newcommand{\notenglish}[1]{\textit{#1}}
\newcommand{\sic}{\notenglish{sic}}
\newcommand{\project}[1]{\textsl{#1}}
\newcommand{\Kepler}{\project{Kepler}}
\newcommand{\name}{PLM}

\begin{document}

\title{%
  A data-driven, pixel-level model to improve the precision of \Kepler\ photometry%
}
\author{%
  Dan~Foreman-Mackey\altaffilmark{\ref{CCPP}},
  David~W.~Hogg\altaffilmark{\ref{CCPP},\ref{MPIA},\ref{email}},
  Tom~Barclay\altaffilmark{\ref{Ames}},
  Rob~Fergus\altaffilmark{\ref{Courant}},
  Stefan Harmeling\altaffilmark{\ref{MPIIS}},
  Bernhard~Sch\"olkopf\altaffilmark{\ref{MPIIS}},
  others%
}

\newcounter{address}
\setcounter{address}{1}
\altaffiltext{\theaddress}{\stepcounter{address}\label{CCPP}%
  Center for Cosmology and Particle Physics, Department of Physics, New York University}
\altaffiltext{\theaddress}{\stepcounter{address}\label{MPIA}%
  Max-Planck-Institut f\"ur Astronomie, Heidelberg, Germany}
\altaffiltext{\theaddress}{\stepcounter{address}\label{email}%
  To whom correspondence should be addressed; \texttt{<david.hogg@nyu.edu>}.}
\altaffiltext{\theaddress}{\stepcounter{address}\label{Ames}%
  NASA Ames Research Center}
\altaffiltext{\theaddress}{\stepcounter{address}\label{Courant}%
  Courant Institute of Mathematical Sciences, New York University}
\altaffiltext{\theaddress}{\stepcounter{address}\label{MPIIS}%
  Max-Planck-Institut f\"ur Intelligente Systeme, T\"ubingen}
%% \altaffiltext{\theaddress}{\stepcounter{address}\label{Oxford}%
%%   Department of Physics, Oxford University}
%% \altaffiltext{\theaddress}{\stepcounter{address}\label{CfA}%
%%   Harvard--Smithsonian Center for Astrophysics}
%% \altaffiltext{\theaddress}{\stepcounter{address}\label{UCL}%
%%   Department of Physics and Astronomy, University College London}
%% \altaffiltext{\theaddress}{\stepcounter{address}\label{CMU}%
%%   McWilliams Center for Cosmology, Carnegie Mellon University}
%% \altaffiltext{\theaddress}{\stepcounter{address}\label{Caltech}%
%%   Department of Astronomy, California Institute of Technology}
%% \altaffiltext{\theaddress}{\stepcounter{address}\label{Columbia}%
%%   Department of Astronomy, Columbia University}

\begin{abstract}
The precision of \Kepler\ photometry%
  ---the most precise photometric measurements of stars ever made---%
  appears to be limited by unknown or untracked variations
  in spacecraft pointing, point-spread function, stellar motions and parallaxes, and temperature.
Here we present \name,
  a data-driven model of these variations in \Kepler\ data (or really of their impact on the photometry).
Importantly (and uniquely),
  \name\ works at the pixel level (not the photometric measurement level);
  it can capture more fine-grained information about the variation of the spacecraft
  (especially regarding pointing and point-spread function)
  than is available in the pixel-summed photometry.
\name\ is extreme in that it has enormous flexibility and it only uses data (and meta-data) to model data:
\name\ provides a prediction for each readout pixel built from a linear combination of the readouts
  from very large numbers of nearby pixels (plus some spacecraft information);
  the choice of nearby pixels is guided by ideas from causal inference.
\name\ avoids over-fitting by employing a train-and-test formalism
  designed to ensure that transit-like photometric events and short-timescale stellar variability
  cannot be captured or distorted by the model;
  it is designed to remove spacecraft-induced variability but not intrinsic stellar variability.
We show that \name\ outperforms the \Kepler\ Presearch (\sic) Data Conditioning method on a set of example stars.
We release open-source code that provides \name\ output for any star in the existing \Kepler\ Archive,
  and we discuss applications for other missions,
  including any possible \Kepler\ two-wheel mission.
\end{abstract}

\section{Ultra-precise photometry}

The photometric measurements of stars made by the \Kepler\ Satellite are precise enough
  to permit discovery of exoplanet transits with depths smaller than $10^{-4}$.
This precision results from great spacecraft stability,
  supplemented by various methods for removing small residual spacecraft-induced and stellar-variability trends in the brightnesses,
  either filtering the data (with things like median filters; CITE)
  or fitting the data with flexible models (like polynomials or splines or Gaussian Processes; CITE).
When employed in the service of exoplanet search and characterization,
  these methods are usually agnostic about whether photometric variations originate in the spacecraft or in the star itself;
  that is, they obliterate intrinsic stellar variability along with spacecraft issues.

In general, there are many reasons for apparent photometric variability in a \Kepler\ source.
There is intrinsic stellar variability,
  which is of interest to some and a nuisance to others.
There is also variability of overlapping fainter stars;
  that is, confusion noise combined with variability of the confusing sources.
There are small changes in spacecraft pointing,
  which leads to slightly different illumination of the focal-plane pixels,
  and thus different sensitivity to errors (problems) in the device flat-field or sensitivity map.
There are also \emph{intra-pixel} sensitivity variations that can contribute (CITE WHITEPAPER, SPITZER).
There are small changes in spacecraft temperature,
  which lead to point-spread function (PSF) and differential (across the focal plane) pointing changes.
These also lead to changes in pixel and intra-pixel illumination.
Stellar proper motion, geometric parallaxes, and differential stellar aberration as the spacecraft orbits all do more of the same.
There is electronic cross-talk between detectors and charge-transfer inefficiency;
  these can effectively transfer variability from one source to another.
There are additional electronics effects like ``rolling bands'' that put additional features into lightcurves.
There are also probably changes to the detector sensitivity with temperature and time,
  possibly illimunation-history effects,
  and possibly sources of variability not yet considered.
The remarkable thing about \Kepler\ is that it is trying to measure stars at a level of precision
  much higher than ever previously attempted;
  new effects really \emph{must} appear at some point.
In \figurename~HOGG, we show the pixel-level variability in the \Kepler\ data
  near one bright star that shows minimal intrinsic variability;
  this figure highlights the spacecraft-induced effects.

We propose and advise dealing with these variations in \Kepler\ light-curves by \emph{modeling} them.
In this context, a model is (at least) a parameterized function that can predict data, given parameter settings,
  and an objective function that can be optimized or sampled to choose or set those parameters.
Ideally the objective function is probabilistic, or has a probabilistic justification,
  as with a likelihood or a posterior pdf.
The model can be a physical model (of the spacecraft PSF, pointing, temperature, and so on)
  or it can be a flexible, effective model that has no direct interpretation in terms of physical spacecraft parameters.
In principle a physical model will do a better job,
  because it necessarily embodies more prior information,
  but it requires research and intuition about dominant effects,
  and this research and intuition is often wrong or incomplete.
The model we propose here is in the non-physical, effective category.
The \Kepler\ community is familiar with these kinds of models;
  to our knowledge, \emph{all} successful light-curve ``de-trending'' methods
  are flexible, effective models.

One such method---one that is designed to model spacecraft-induced problems
  but \emph{not} interfere with measurements of stellar variability---%
  is the \Kepler\ Presearch (\sic) Data Conditioning (PDC) method (CITE).
The PDC ``de-trends'' the \Kepler\ lightcurves by fitting them with a small set of basis lightcurves
  generated from a principal components analysis (PCA) of filtered lightcurves.
That is, it uses data to model data,
  regularizing the fit (and avoiding over-fitting) by filtering and restricting the dimensionality (through PCA).
The method proposed here, \name, is very similar in spirit to the PDC.
The main differences are
  (1)~that \name\ works in the pixel domain, not the lightcurve domain, so it has access to more fine-grained information,
  (2)~that \name\ has far more freedom (far more parameters) than the PDC
  but it strictly avoids over-fitting the lightcurves on exoplanet-transit time-scales
  through strong regularization and a train-and-test framework, and
  (3)~that when \name\ fits a star, there is no possibility of any contribution of the star itself to the fit basis vectors.
The two methods (\name\ and PDC) are similar however,
  in that they both make the assumption that whatever spacecraft effects are imprinting variability on a stellar lightcurve
  must also be imprinting similar or related variability on other lightcurves.
They also assume that the relationships can be captured with linear models.
By going to the pixel level (unlike the PDC, which works at the photometry level),
  \name\ makes it easier for the model to capture variability
  that is coming through variations in the centroids and point-spread function
  from spacecraft pointing, roll, and temperature.

Before we start, a few reminders about the \Kepler\ data are in order:
The satellite always observes precisely the same field, at fixed pointing (as closely as possible).
The satellite is rolled by 90~deg every 90-ish days (for Solar Angle constraints).
The PSF varies strongly across the field and is badly sampled.
The stars span a huge range in brightness;
  some of \Kepler's most important targets even saturate the device and bleed charge.
The stellar photometry returned by the \Kepler\ SAP and PDC pipelines is based on
  straight, \emph{unweighted} sums of pixels in small patches centered on the stellar centroid.
We will return to this latter point at the end;
  this kind of photometry cannot be optimal;
  it must be possible to do a better job with the photometric measurements.
That's beyond the scope of this paper but a place for a valuable intervention on the \Kepler\ data.

The scope of this paper is an intervention---\name---that makes the \Kepler\ photometry more precise.
We describe the method and deliver all the relevant code in a public, open-source repository.
We also provide an interface to the \Kepler\ data that delivers improved ``\name\ photometry''
  for every \Kepler\ target.

\section{Generalities: Data-driven models}

The first general idea about data-driven modeling of the kind used here
  is that each data point or data source is going to be modeled
  (or really \emph{predicted})
  with a parameterized mathematical function of \emph{other data}.
That is, given a choice of some \emph{target} \Kepler\ data,
  we are going to find parameters of a function that takes as input some of the other \Kepler\ data,
  and provides as output predictions of the target data.
The simplest models are \emph{linear models},
  in which predictions for the target data are built from linear combinations of the other data,
  and in which the objective functions are quadratic in the prediction residual.
Examples of quadratic objective functions include Gaussian likelihoods, mean-squared-error, and chi-square statistics.
These models are simple,
  not just because they are easy to express and compute,
  but because optimization is convex:
There is only one optimum for the objective function.

The point of these models is to be \emph{flexible},
  so the usual approach is to make the input data set very large,
  and the number of parameters large,
  often as large as---or larger than---the target data available.
Such fits require \emph{regularization} to break degeneracies
  and control ill-constrained parameters.
These regularizations do for optimizations what priors do for posterior probability inferences;
  they express the desired behavior of the fit in the absence of data
  or along directions in parameter space in which the data are not constraining.
General regularizations or priors will break the convexity of linear models;
  if convexity is to be maintained, these regularizations also have to be quadratic,
  or else one of a small number of other known forms (one of which is L1-regularization).
Given these considerations, it makes sense to try to build our pixel-level model
  using a linear model with a quadratic objective function and a quadratic regularization.

[General formula here for a pixel predicted using a linear combination of other pixels.
  And the objective function, generally?]

The second general idea about data-driven modeling is the investigator's beliefs
  about the causal processes that generate the data
  are crucial in restricting the kinds of data that can be used as input to the model.
That is, different assumptions about the physical properties of the data
  and the data-taking system
  lead to different structures for the data-driven model.
For example, in the case of \Kepler\ data,
  if we believe that different stars in the \Kepler\ field vary independently
  (that is, are not physically synchronized in any way),
  then the only reason that one star might show variability that is strongly \emph{predictive}
  (useful for prediction) of another star's variability
  is that both stars are being observed by the same device or spacecraft.
That is, one star's pixels can be used to predict another star's pixels
  inasmuch as spacecraft issues imprint on both stars in related ways;
  they share a common cause.
For another example, if we think the spacecraft is being affected by
  processes that take place over time-scales longer than a single read-out
  (for example, thermal processes),
  then it would be sensible to model the data at time $t$ using not just simultaneous data,
  but also data from a range of times around $t$.
For another example, if an investigator doesn't care about preserving stellar variability,
  and just wants to detect exoplanet transits (say),
  pixels from the target star can be used to predict pixels from the target star,
  provided they are at large-enough time lags that they don't overlap (in time)
  the signals of interest.
That is, if the model is designed to fit not just spacecraft variability
  but also intrinsic stellar variability,
  the predictive model will be permitted to use as input pixels that \emph{do} overlap the target star.
In what follows, we are only going to use input data that do not overlap that target star pixels.
These models ought to remove spacecraft variability but not intrinsic stellar variability.

The third general idea to mention is that we need to control for over-fitting.
That is, once you have a flexible-enough model you can in principle fit \emph{anything},
  whether it was caused by the spacecraft, intrinsic stellar variability, or a transiting exoplanet.
How do you prevent a very flexible model from using small noise-induced fluctuations in all the input data
  from being carefully combined linearly into models for every nook and cranny in the target data?
In most projects in astrophysics, over-fitting is controlled for by limiting model freedom.
The model is restricted in the number of parameters
  (as in ``you can't have more parameters than data points'')
  or by limiting the dimensionality
  (as in principal components analysis)
  or by applying strong priors
  (as with smoothness priors, that effectively reduce freedom without explicitly reducing the number of parameters).
In each case, the restriction on the model can be set with tools like cross-validation,
  the fully marginalized likelihood (Bayes factor or evidence),
  chi-squared statistical criteria,
  or intuition or heuristics.
Here we take a different and more general approach, which is to use a train-and-test framework.

In this framework, the data used to \emph{train} the model%
  ---meaning set the values of the parameters of the model---%
  are disjoint from the \emph{test} data
  ---meaning the data that are being predicted (the target data).
Because we are extremely concerned with detecting Earth-analog exoplanet transits,
  which take about ten hours,
  we adopt an extreme version of the train-and-test framework,
  in which the training data are always separated from the test data by at least ten hours.
That is, when we are using the model to predict a particular pixel in the target data taken at time $t$,
  we use parameters obtained by an optimization that makes use of only data
  that comes either at times prior to $t-\Delta t$ or after $t+\Delta t$,
  where $\Delta t$ is a tunable parameter but which we will set to roughly $10$~h.
This ensures that no information about any exoplanet transit itself
  can be entering into the prediction of the pixels contributing to the stellar photometry.
In general, if there is a scientific goal of preserving intrinsic stellar variability,
  or transit signals,
  on time-scales of $\tau$,
  the parameter optimization ought to be based on training data taken with a time-exclusion zone of width $\Delta > \tau$.

In addition, each training data set within which we set the parameters (by optimization) has a finite total time extent $t_{\max}$.
The train-and-test framework, which includes data out to time $t_{\max}$ but excludes data within $\Delta t$ of the test data,
  effectively assumes that the signals worth preserving have time-scales less than $\Delta t$ but don't recur on time-scales
  shorter than $t_{\max}$.
Those assumptions are good for our purposes but not necessarily ideal for all users:
Short-period exoplanets and certain kinds of stellar variability
  could be wiped out by a model with these settings of the training-data regime.

Fourth idea: predict; residuals are our signal...

Fifth idea: no way to know if we are \emph{right}...

\section{Specifics: PLM and model hyper-parameters}

\section{Examples and results}

\section{Discussion}

Return to the SAP photometry ``unweighted sum'' issue here.

Didn't do anything about short-cadence data; suck it up.

Lots of quantitative and qualitative choices that ought to be explored.

Not the least of these is linearity.

Interesting whether the output of the model can be interrogated to provide value for those building physical models.

\acknowledgements
It is a pleasure to thank the whole \Kepler\ Team
  for designing, delivering, and operating a great facility,
  and for making all of the data public, in all its rawest forms, through the MAST interface.
We are also pleased to thank
  Ruth~Angus (Oxford),
  Bekki~Dawson (Berkeley),
  Michael~Hirsch (UCL),
  Dustin~Lang (CMU),
  Benjamin~T.~Montet (Caltech),
  and
  David~Schiminovich (Columbia)
for valuable discussions, input, encouragement, and advice.

WHITEPAPER AUTHORS:  FEEL FREE TO CONTRIBUTE TO THIS PAPER AND MOVE FROM THE ACK LIST TO THE AUTHOR LIST.

\end{document}
