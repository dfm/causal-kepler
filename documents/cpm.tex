% This file is part of the causal-kepler project
% Copyright 2013, 2014 the authors.

%

\documentclass[12pt, preprint]{aastex}

\newcommand{\notenglish}[1]{\textit{#1}}
\newcommand{\sic}{\notenglish{sic}}
\newcommand{\project}[1]{\textsl{#1}}
\newcommand{\Kepler}{\project{Kepler}}
\newcommand{\name}{CPM}
\newcommand{\set}[1]{\mathcal{#1}}
\newcommand{\given}{\,|\,}
\newcommand{\todo}[1]{\textbf{#1}}

\begin{document}

\title{%
  Self-calibration of time-domain imaging for exoplanet science, informed by causal structure%
  \\ or \\
  A data-driven, pixel-level model to improve the precision of \Kepler\ photometry%
  \\ or \\
  Exploiting causal structure to create a data-driven self-calibration of \Kepler\ imaging%
}
\author{%
  Dun~Wang\altaffilmark{\ref{CCPP}},
  Dan~Foreman-Mackey\altaffilmark{\ref{CCPP}},
  David~W.~Hogg\altaffilmark{\ref{CCPP},\ref{CDS},\ref{MPIA},\ref{email}},
  Bernhard~Sch\"olkopf\altaffilmark{\ref{MPIIS}},
  others%
}
\newcounter{address}
\setcounter{address}{1}
\altaffiltext{\theaddress}{\stepcounter{address}\label{CCPP}%
  Center for Cosmology and Particle Physics, Department of Physics, New York University}
\altaffiltext{\theaddress}{\stepcounter{address}\label{CDS}%
  Center for Data Science, New York University}
\altaffiltext{\theaddress}{\stepcounter{address}\label{MPIA}%
  Max-Planck-Institut f\"ur Astronomie, Heidelberg, Germany}
\altaffiltext{\theaddress}{\stepcounter{address}\label{email}%
  To whom correspondence should be addressed; \texttt{<david.hogg@nyu.edu>}.}
%% \altaffiltext{\theaddress}{\stepcounter{address}\label{Ames}%
%%   NASA Ames Research Center}
%% \altaffiltext{\theaddress}{\stepcounter{address}\label{Courant}%
%%   Courant Institute of Mathematical Sciences, New York University}
\altaffiltext{\theaddress}{\stepcounter{address}\label{MPIIS}%
  Max-Planck-Institut f\"ur Intelligente Systeme, T\"ubingen}
%% \altaffiltext{\theaddress}{\stepcounter{address}\label{Oxford}%
%%   Department of Physics, Oxford University}
%% \altaffiltext{\theaddress}{\stepcounter{address}\label{CfA}%
%%   Harvard--Smithsonian Center for Astrophysics}
%% \altaffiltext{\theaddress}{\stepcounter{address}\label{UCL}%
%%   Department of Physics and Astronomy, University College London}
%% \altaffiltext{\theaddress}{\stepcounter{address}\label{CMU}%
%%   McWilliams Center for Cosmology, Carnegie Mellon University}
%% \altaffiltext{\theaddress}{\stepcounter{address}\label{Caltech}%
%%   Department of Astronomy, California Institute of Technology}
%% \altaffiltext{\theaddress}{\stepcounter{address}\label{Columbia}%
%%   Department of Astronomy, Columbia University}

\begin{abstract}
The precision of \Kepler\ photometry for exoplanet science---the most
precise photometric measurements of stars ever made---appears to be
limited by unknown or untracked variations in spacecraft pointing and
temperature, and unmodeled stellar variability.
Here we present \name,
  a data-driven model of these variations in \Kepler\ data (or really of their impact on the photometry).
Importantly (and uniquely),
  \name\ works at the pixel level (not the photometric measurement level);
  it can capture more fine-grained information about the variation of the spacecraft
  (especially regarding pointing and point-spread function)
  than is available in the pixel-summed photometry.
\name\ is extreme in that it has enormous flexibility and it only uses data (and meta-data) to predict data:
\name\ provides a prediction for each readout pixel built from a linear combination of the readouts
  from very large numbers of nearby pixels (plus some spacecraft information);
  the choice of nearby pixels is guided by ideas from causal inference.
\name\ avoids over-fitting by employing a train-and-test formalism
  designed to ensure that transit-like photometric events and short-timescale stellar variability
  cannot be captured or distorted by the model;
  it is designed to remove spacecraft-induced variability but not intrinsic stellar variability.
We show that \name\ outperforms the \Kepler\ Presearch (\sic) Data Conditioning method on a set of example stars.
We release open-source code that provides \name\ output for any star in the existing \Kepler\ Archive,
  and we discuss applications for other missions,
  including any possible \Kepler\ two-wheel mission.
\end{abstract}

\section{Ultra-precise photometry}

The photometric measurements of stars made by the \Kepler\ Satellite are precise enough
  to permit discovery of exoplanet transits with depths smaller than $10^{-4}$.
This precision results from great spacecraft stability,
  supplemented by various methods for removing small residual spacecraft-induced and stellar-variability trends in the brightnesses,
  either filtering the data (with things like median filters; \todo{citations})
  or fitting the data with flexible models (like polynomials or splines or Gaussian Processes; \todo{citations}).
When employed in the service of exoplanet search and characterization,
  these methods are usually agnostic about whether photometric variations originate in the spacecraft or in the star itself;
  that is, they obliterate intrinsic stellar variability along with spacecraft issues.

In general, there are many reasons for apparent photometric variability in a \Kepler\ source.
There is intrinsic stellar variability,
  which is of interest to some and a nuisance to others.
There is also variability of overlapping fainter stars;
  that is, confusion noise combined with variability of the confusing sources.
There are small changes in spacecraft pointing,
  which leads to slightly different illumination of the focal-plane pixels,
  and thus different sensitivity to errors (problems) in the device flat-field or sensitivity map.
There are also \emph{intra-pixel} sensitivity variations that can contribute (\todo{cite white paper; Spitzer results}).
There are small changes in spacecraft temperature,
  which lead to point-spread function (PSF) and differential (across the focal plane) pointing changes.
These also lead to changes in pixel and intra-pixel illumination.
Stellar proper motion, geometric parallaxes, and differential stellar aberration as the spacecraft orbits all do more of the same.
There is electronic cross-talk between detectors and charge-transfer inefficiency;
  these can effectively transfer variability from one source to another.
There are additional electronics effects like ``rolling bands'' that put additional features into lightcurves.
There are also probably changes to the detector sensitivity with temperature and time,
  possibly illimunation-history effects,
  and possibly sources of variability not yet considered.
The remarkable thing about \Kepler\ is that it is trying to measure stars at a level of precision
  much higher than ever previously attempted;
  new effects really \emph{must} appear at some point.
In \figurename~\ref{fig:pixelpatch}, we show the pixel-level variability in the \Kepler\ data
  near one bright star that shows minimal intrinsic variability;
  this figure highlights the spacecraft-induced effects.

We propose and advise dealing with these variations in \Kepler\ light-curves by \emph{modeling} them.
In this context, a model is (at least) a parameterized function that can predict data, given parameter settings,
  and an objective function that can be optimized or sampled to choose or set those parameters.
Ideally the objective function is probabilistic, or has a probabilistic justification,
  as with a likelihood or a posterior pdf.
The model can be a physical model (of the spacecraft PSF, pointing, temperature, and so on)
  or it can be a flexible, effective model that has no direct interpretation in terms of physical spacecraft parameters.
In principle a physical model will do a better job,
  because it necessarily embodies more prior information,
  but it requires research and intuition about dominant effects,
  and this research and intuition is often wrong or incomplete.
The model we propose here is in the non-physical, effective category.
The \Kepler\ community is familiar with these kinds of models;
  to our knowledge, \emph{all} successful light-curve ``de-trending'' methods
  are flexible, effective models.

One such method---one that is designed to describe or model spacecraft-induced problems
  but \emph{not} interfere with measurements of stellar variability---%
  is the \Kepler\ Presearch (\sic) Data Conditioning (PDC) method (\todo{citation}).
The PDC ``de-trends'' the \Kepler\ lightcurves by fitting them with a small set of basis lightcurves
  generated from a principal components analysis (PCA) of filtered lightcurves.
That is, it uses data to predict data,
  regularizing the fit (and avoiding over-fitting) by filtering and restricting the dimensionality (through PCA).
The method proposed here, \name, is very similar in spirit to the PDC.
The main differences are
  (1)~that \name\ works in the pixel domain, not the lightcurve domain, so it has access to more fine-grained information,
  (2)~that \name\ has far more freedom (far more parameters) than the PDC
  but it strictly avoids over-fitting the lightcurves on exoplanet-transit time-scales
  through strong regularization and a train-and-test framework, and
  (3)~that when \name\ fits a star, there is no possibility of any contribution of the star itself to the fit basis vectors.
The two methods (\name\ and PDC) are similar however,
  in that they both make the assumption that whatever spacecraft effects are imprinting variability on a stellar lightcurve
  must also be imprinting similar or related variability on other lightcurves.
They also assume that the relationships can be captured with linear models.
By going to the pixel level (unlike the PDC, which works at the photometry level),
  \name\ makes it easier for the model to capture variability
  that is coming through variations in the centroids and point-spread function
  from spacecraft pointing, roll, and temperature.

Before we start, a few reminders about the \Kepler\ data are in order:
The satellite always observes precisely the same field, at fixed pointing (as closely as possible).
Only a few percent of the 80 million pixels in the focal plane are telemetered down
  to Earth from each 30-min exposure;
  the telemetered pixels are associated with \Kepler\ target stars chosen for study by the \Kepler\ Team.
The satellite is rolled by 90~deg every 90-ish days (to satisfy Solar-Angle constraints).
The focal plane contains many CCDs;
  this plus the 90-deg rotations means that each star is at a particular location
  on a particular CCD for 90-ish-day contiguous periods of time.
The PSF varies strongly across the field and is badly sampled.
The stars span a huge range in brightness;
  some of \Kepler's most important targets even saturate the device and bleed charge.
The stellar photometry returned by the \Kepler\ SAP and PDC pipelines is based on
  straight, \emph{unweighted} sums of pixels in small patches centered on the stellar centroid.
We will return to this latter point at the end;
  this kind of photometry cannot be optimal;
  it must be possible to do a better job with the photometric measurements.
That's beyond the scope of this project but a place for a valuable intervention on the \Kepler\ data.

The scope of this project is an intervention---\name---that makes the \Kepler\ photometry more precise.
We describe the method and deliver all the relevant code in a public, open-source repository.
We also provide an interface to the \Kepler\ data that delivers improved ``\name\ photometry''
  for every \Kepler\ target.

\section{Data-driven models}

The first general idea about data-driven modeling of the kind used here
  is that each data point or data source is going to be 
  \emph{predicted} with or using a parameterized mathematical function of \emph{other data}.
That is, given a choice of some \emph{target} \Kepler\ data,
  we are going to find parameters of a function that takes as input some of the other \Kepler\ data,
  and provides as output predictions of the target data.
The simplest models are \emph{linear models},
  in which predictions for the target data are built from linear combinations of the other data,
  and in which the objective functions are quadratic in the prediction residual.
Examples of quadratic objective functions include Gaussian likelihoods, mean-squared-error, and chi-square statistics.
These models are simple,
  not just because they are easy to express and compute,
  but because optimization is convex:
There is only one optimum for the objective function.

The point of these models is to be \emph{flexible},
  so the usual approach is to make the input data set very large,
  and the number of parameters large,
  often as large as---or larger than---the target data available.
Such fits require \emph{regularization} to break degeneracies
  and control ill-constrained parameters.
These regularizations do for optimizations what priors do for posterior probability inferences;
  they express the desired behavior of the fit in the absence of data
  or along directions in parameter space in which the data are not constraining.
General regularizations or priors will break the convexity of linear models;
  if convexity is to be maintained, these regularizations also have to be quadratic,
  or else one of a small number of other known forms (one of which is L1-regularization).
Given these considerations, it makes sense to try to build our pixel-level model
  using a linear model with a quadratic objective function and a quadratic regularization;
  this is what \name\ will be.

The second general idea about data-driven modeling is the investigator's beliefs
  about the causal processes that generate the data
  are crucial in restricting the kinds of data that can be used as input to the model.
That is, different assumptions about the physical properties of the data
  and the data-taking system
  lead to different structures for the data-driven model.
For example, in the case of \Kepler\ data,
  if we believe that different stars in the \Kepler\ field vary independently
  (that is, are not physically synchronized in any way),
  then the only reason that one star might show variability that is strongly \emph{predictive}
  (useful for prediction) of another star's variability
  is that both stars are being observed by the same device or spacecraft.
That is, one star's pixels can be used to predict another star's pixels
  inasmuch as spacecraft issues imprint on both stars in related ways;
  they share a common cause.
For another example, if we think the spacecraft is being affected by
  processes that take place over time-scales longer than a single read-out
  (for example, thermal processes),
  then it would be sensible to model the data at time $t$ using not just simultaneous data,
  but also data from a range of times around $t$.
For another example, if an investigator doesn't care about preserving stellar variability,
  and just wants to detect exoplanet transits (say),
  pixels from the target star can be used to predict pixels from the target star,
  provided they are at large-enough time lags that they don't overlap (in time)
  the signals of interest.
That is, if the model is designed to fit not just spacecraft variability
  but also intrinsic stellar variability,
  the predictive model will be permitted to use as input pixels that \emph{do} overlap the target star.
In what follows, we are only going to use input data that do not overlap that target star pixels.
These models ought to remove spacecraft variability but not intrinsic stellar variability.

The third general idea is that we need to control for over-fitting.
That is, once you have a flexible-enough model you can in principle fit \emph{anything},
  whether it was caused by the spacecraft, intrinsic stellar variability, or a transiting exoplanet.
How do you prevent a very flexible model from taking small noise-induced fluctuations in all the input data
  and carefully combining them linearly into detailed models for every nook and cranny in the target data?
In most projects in astrophysics, over-fitting is controlled for by limiting model freedom.
The model is restricted in the number of parameters
  (as in ``you can't have more parameters than data points'')
  or by limiting the dimensionality
  (as in principal components analysis)
  or by applying strong priors
  (as with smoothness priors, that effectively reduce freedom without explicitly reducing the number of parameters).
In each case, the restriction on model freedom is controlled by some \emph{hyper-parameter}
  (such as the number of inputs, or number of principal components, or the strength of the smoothness prior).
The hyper-parameters can be set or tested with tools like cross-validation,
  the fully marginalized likelihood (Bayes factor or evidence),
  chi-squared statistical criteria,
  or intuition or heuristics.
Here we take a different and more general approach, which is to use a train-and-test framework.

In this framework, the data used to \emph{train} the model%
  ---meaning set the values of the parameters of the model---%
  are disjoint from the \emph{test} data
  ---meaning the data that are being predicted (the target data).
Because we are extremely concerned with detecting Earth-analog exoplanet transits,
  which take about ten hours,
  we adopt an extreme version of the train-and-test framework,
  in which the training data are always separated from the test data by at least ten hours.
That is, when we are using the model to predict a particular pixel in the target data taken at time $t$,
  we use parameters obtained by an optimization that makes use of only data
  that comes either at times prior to $t-\Delta t$ or after $t+\Delta t$,
  where $\Delta t$ is a tunable parameter but which we will set to roughly $12$~h.
This ensures that no information about any exoplanet transit itself
  can be entering into the prediction of the pixels contributing to the stellar photometry.
In general, if there is a scientific goal of preserving intrinsic stellar variability,
  or transit signals,
  on time-scales of $\tau$,
  the parameter optimization ought to be based on training data taken with a time-exclusion zone of half-width $\Delta t > \tau$.

In addition, each training data set within which we set the parameters (by optimization) has a finite total time extent $t_{\max}\approx 90$\,d.
The train-and-test framework, which includes data out to time $t_{\max}$ but excludes data within $\Delta t$ of the test data,
  effectively assumes that the signals worth preserving have time-scales less than $\Delta t$ but don't recur on time-scales
  shorter than $t_{\max}$.
Those assumptions are good for our purposes but not necessarily ideal for all users:
Short-period exoplanets and certain kinds of stellar variability
  could be wiped out by a model with these settings of the training-data regime.

The fourth general idea involved in this kind of data-driven modeling is that the models aren't \emph{interpretable}.
This means, in particular, that although the model might do a good job of \emph{predicting} the target pixels,
  using a linear (or more complex) combination of the input pixel data,
  it won't deliver anything that can be unambiguously interpreted as the \emph{flux} of the star in question,
  or any other signal we care about.
The data-driven model \emph{effectively} describes the pointing, point-spread function, and flat-field
  of the telescope and camera,
  plus the variability of every star and every exoplanet transit,
  but it does so without ever \emph{explicitly} creating any of those objects.
We have to make some kind of heuristic or interpretive move to extract from the data-driven model the quanitites of interest.

In the context of this project, the quantities of interest are the light-curves
  (photometry as a function of time or apparent flux as a function of time)
  for every star.
What we get, for every star, from the data-driven model, is a train-and-test prediction
  for the brightness of every pixel in the patch of the detector relevant to the star,
  along with the residual, or the difference between the measured read-out pixel value and the prediction (data minus prediction).
What will be called the ``\name\ photometry'' in what follows is constructed from what will be called the ``\name\ pixels''.
The \name\ photometry is constructed from the \name\ pixels
  just as the \Kepler\ official SAP photometry is constructed from the \Kepler\ Target Pixel Files;
  that is, by summing pre-defined sets of pixels (with unit weights; see above and below for criticism of this).
The \name\ pixels are constructed from the data-driven model predictions by taking,
  for each pixel in each contiguous data segment (see below),
  the \emph{mean} predicted pixel value (mean over time),
  and adding to it the residuals (data minus prediction) away from the data-driven model prediction.
These \name\ pixels are ``calibrated'' pixels,
  in that the data-driven model has been used to fit out all the variability it can,
  subject to the restrictions imposed by its finite freedom and the train-and-test specification.
The signal---in which exoplanet transits will be found---is fundamentally the observed residual away from this prediction,
  but it has to be added back into the mean prediction to have the right ``baseline'' or zero level.
In short, \name\ will produce prediction pixels, residual pixels, \name\ pixels (mean prediction plus residual),
  and aperture-based \name\ photometry analogous to the SAP photometry.

Finally, the fifth general idea is that there is no objective \emph{ground truth} against which we can tell
  that any particular data-driven model is better or worse than any other.
This problem is a problem for \name, and for the \Kepler\ PDC, and any other data-driven calibration models.
One might think that the ``best'' model is the one that predicts data with lowest variance;
  this would be true if all models adhered to the same train-and-test framework, which they don't.
Besides, a model that can predict not just the spacecraft-induced variability,
  but also variability caused by exoplanet transits of interest,
  will effectively over-fit and distort the most important information in the light-curves.
That is, what is considered best for modeling the data depends on the objectives of the user.
For us, who are interested (in the long term) in finding and characterizing Earth analogs,
  the ``best'' data-driven model is the one that produces the most success in finding and characterizing Earth analogs!
What we will show in what follows is that the \name\ photometry does not distort
  artificial exoplanet transit signals injected into real \Kepler\ pixel-level data.
We can also show that it produces stellar light-curves that look sensible for known variable stars.
We cannot yet show that it is optimal for our long-term goals,
  nor can we show that it produces pixelss or light-curves that are more accurately calibrated;
  we will only show promising properties of the \name\ outputs.
In the end, the value of the \name\ will be demonstrated by the scientific projects it enables.

\section{\name\ specification and hyper-parameters}

Each individual \Kepler\ target star is, for each quarter (each 90-ish-day period),
  on a particular CCD on the \Kepler\ focal plane.
Associated with each target star is some set of pixels,
  located in a small contiguous patch more-or-less centered on the target star,
  and telemetered down once for every 30-min exposure.
(Here we are ignoring all short-cadence data that is taken in a different mode with shorter exposures.)
That is, each telemetered pixel in each CCD is associated with a particular \Kepler\ target star.

Let's consider a particular pixel $m$ in the focal plane in one particular quarter.
In that quarter, the spacecraft delivers $N\sim 4000$ measurements $I_{mn}$
  of the intensity falling in that pixel at the $N$ times $t_n$ at which exposures were taken during the quarter.
(For intensity measurements $I_{mn}$ here we are using pixels from the Target Pixel Files.)
We want to build a prediction for the intensity $I_{mn}$ in pixel $m$ at each time $t_n$
  using the intensities $I_{m'n}$ of other pixels $m'$.
The question is:  What other pixels to choose?
There are many possible qualitatively and quantitatively different choices here.
Somewhat arbitrarily,
  we choose all the pixels $m'$ associated with the $Q=5$ stars on the same CCD in the same quarter
  that are closest in magnitude
  (\Kepler\ magnitude as reported in the \Kepler\ Input Catalog)
  to the target star associated with pixel $m$.
This set of pixels%
  ---from the $Q$ stars on the same CCD---%
  is the \emph{pixel set} $\set{M}_m$ associated with pixel $m$.
Note that because the set $\set{M}_m$ is of pixels associated with \emph{different} stars
  than the star associated with pixel $m$,
  the pixel $m$ will not be in the set $\set{M}_m$,
  and nor will any of pixel $m$'s close neighbors.
That is, there will be no (or almost no) overlap in stellar illumination of pixel $m$
  and the pixels $m'\in\set{M}_m$.

When we are predicting the measurement $I_{mn}$ from pixel $m$ at a particular time $t_n$,
  we are going to use a train-and-test framework in which not only do we not use
  data at time $t_n$ in optimizing the parameters of the regression or fit,
  but we don't even use any times $t$ such that $|t-t_n| < \Delta t$,
  where $\Delta t\sim 12$\,h.
That is, we train (optimize fit parameters) using the \emph{time set} $\set{N}_n$ of time
  indices $n'$ such that for all $n'\in\set{N}_n$,
  $t_{n'}$ is in the same quarter as $t_n$,
  and $|t_{n'} - t_n|>\Delta t$.
The time set of indices $\set{N}_n$ therefore does not overlap the time point $t_n$,
  nor any of its neighbors in a time window of half-width $\Delta_t$.

\todo{DFM: Do we want a schematic figure or cartoon which illustrates all of this?}

In addition to the other data (from pixels $m'\in\set{M}_m$ and taken at time points $n'\in\set{N}_n$)
  used to make each prediction, we will use also some \emph{meta-data};
  these include the time itself $t_n$,
  the sine and cosine of the parallactic angle $\theta(t_n)$,
  and the dot product $\hat{n}\cdot v(t_n)$ of the spacecraft velocity $v(t_n)$
  with the direction $\hat{n}$ towards the \Kepler\ field center.
These additional meta-data variables permit the data-driven model
  to capture easily variations induced in the data
  by proper motions, parallaxes, and stellar aberration.
Symbolically, we will denote these $K=4$ pieces of meta-data $d_{kn}$.

As mentioned above, in the context of this project,
  a model is something that can predict data given settings of some parameters,
  and an objective function that can be used to find best values for those parameters.
We prefer to work in a probabilistic mode in which the objective function can be justified
  in terms of a likelihood (probability for the data given the model)
  or a posterior probability density function (likelihood times a prior pdf).
The \name\ treats each \Kepler\ data point $I_{mn}$ as being
  predictable from a linear combination of data points $I_{m'n}$,
  where $m'$ is from the set of non-overlapping pixels $m'\in\set{M}_m$.
\begin{eqnarray}
I_{mn}         &=& I^{\ast}_{mn} + e_{mn}
\\
I^{\ast}_{mn}  &=& \sum_{m'\in\set{M}_m} a_{mnm'}\,I_{m'n} + \sum_{k=1}^{K} b_{mnk}\,d_{kn}
\quad,
\end{eqnarray}
where $I^{\ast}_{mn}$ is the prediction (by the model) for data point $I_{mn}$,
  $e_{mn}$ is a noise contribution or residual away from the prediction,
  and the $a_{mnm'}$ and $b_{mnk}$ are the parameters (linear coefficients of the prediction).
The parameters $a_{mnm'}$ and $b_{mnk}$ have indices $m$ because
  they are different for every pixel $m$,
  and they will even be different for every time step $n$;
  that is, there will be a separate best-fit value for the parameters for every $I_{mn}$.
If we presume that the residuals away from the prediction are normally distributed with zero mean
  and known variance,
  likelihood optimization reduces to chi-squared minimization.
We add to the standard chi-squared definition a regularization term
  (equivalent to multiplying the likelihood by a Gaussian prior pdf)
  that penalizes large absolute values for the the coefficients $a_{mnm'}$:
\begin{eqnarray}
\chi^2_{mn}    &=& \sum_{n'\in\set{N}_n} \frac{[I_{mn'} - I^{\ast}_{mnn'}]^2}{\sigma^2_{mn'}}
                 + \sum_{m'\in\set{M}_m} \frac{a_{mnm'}^2}{\Sigma^2}
\\
I^{\ast}_{mnn'} &=& \sum_{m'\in\set{M}_m} a_{mnm'}\,I_{m'n'} + \sum_{k=1}^{K} b_{mnk}\,d_{kn'}
\quad,
\end{eqnarray}
where the $\sigma^2_{mn'}$ are the (presumed known) individual-pixel noise variances,
  and $\Sigma^2$ sets the strength of the regularization (or width of the prior pdf).
Although the variances $\sigma^2_{mn'}$ are in principle different for every time step $n'$,
  for nearly-constant stars in good data, there is not much variation;
  in what follows, we treat these uncertainties as fixed at
  \todo{DFM: some mean value (we can't ignore this,
   because it ``plays off against'' the regularization)}
The regularization variance $\Sigma^2$ we set to 0.3 (\todo{DFM TAKE NOTE; that's about 10 for the parameter you usually use.})
  because we expect almost all the coefficients $a_{mnm'}$ to be less than unity in magnitude.
We do not regularize the meta-data coefficients $b_{mnk}$,
  because we want the model to make use of the (easily interpretable) meta-data coefficients as much as possible.

The train-and-test idea comes into the objective function $\chi^2_{mn}$:
When we are computing the objective function $\chi^2_{mn}$,
  we are using only time points $n'\in\set{N}_n$ that don't overlap the target (or test) time point $n$.
We train the model---that is, set the parameters $a_{mnm'}$ and $b_{mnk}$---%
  by choosing the full set of parameters that jointly minimize the objective function $\chi^2_{mn}$.
Fortunately, given the form of the model,
  this minimization is just a linear solve (linear operation on the data).
Importantly however, and perhaps surprisingly, the objective function $\chi^2_{mn}$ is \emph{different}
  for every target data point (pixel value to be predicted) $I_{mn}$;
  we have to do an \emph{independent optimization} (or really linear solve)
  of the parameters for every pixel value we want to predict.
That is, every pixel datum $I_{mn}$ we predict will have
  \emph{different} settings of the parameters $a_{mnm'}$ and $b_{mnk}$.
Since there are of order $10^{6}$ total pixel values in the \Kepler\ Target Pixel Files for \emph{each \Kepler\ target},
  and of order $10^7$ data sources used as basis functions in the linear fitting,
  this represents one fuck of a lot of linear solves,
  and a lot of parameters to obtain and record.

For this reason---\emph{so many solves}---%
  we do \todo{some crazy clever stuff} to make this work fast;
  \todo{DFM implementation notes here}.

Once we have obtained the parameters $a_{mnm'}$ corresponding to a particular data point $I_{mn}$,
  we can make a \emph{prediction} $I^{\ast}_{mnn}$ for the intensity at that pixel.
By construction of our objective function, this is truly a prediction,
  in the sense that the optimization (or solve) that produced the parameters
  (and thus the prediction)
  did not make use of $I_{mn}$ itself,
  or even any of the pixel values that are nearby in angle or time
  (as described above).
In addition to the predicted pixel value $I^{\ast}_{mnn}$,
  we can also construct a \emph{residual} $\delta_{mn}$ (data minus prediction),
  defined by
\begin{eqnarray}
\delta_{mn} &\equiv& I_{mn} - I^{\ast}_{mnn}
\quad .
\end{eqnarray}

In some sense, it is this residual that contains the exoplanet transit signals we seek.
That is, a transit creates a negative residual away from the prediction.
So far, however, the model is a model of \emph{pixels},
  and we need to obtain from this model \emph{photometric measurements} of stars.
In order to construct stellar photometry measurements out of the residual pixels $\delta_{mn}$,
  we add into each pixel residual $\delta_{mn}$ the \emph{mean predicted intensity} $\bar{I}^{\ast}_m$
\begin{eqnarray}
\bar{I}^{\ast}_m &=& \frac{1}{N_m}\,\sum_n I^{\ast}_{mnn}
\quad ,
\end{eqnarray}
where $N_m$ is the number of time steps in the quarter,
  and the sum is over those time steps.
\todo{DFM: Should we use mean or median here?}
We construct what we will call ``the \name\ pixels'' $J_{mn}$,
  which are the residuals plus the mean, or
\begin{eqnarray}
J_{mn} &=& \delta_{mn} + \bar{I}^{\ast}_m
\quad .
\end{eqnarray}
That is, we treat each pixel as differing from constant intensity
  only inasmuch as it has residuals away from the model prediction.

Our new stellar flux estimates,
  what we will call the ``\name\ photometry'',
  is constructed from the \name\ pixels
  in precisely the same way as the official \Kepler\ SAP photometry
  is constructed from the Target Pixel Files.
That is, we perform a weighted sum of \name\ pixels $J_{mn}$ with weights $w_m$,
  all of which are either zero or unity,
  and we adopt precisely the same weight assignments as are adopted in the SAP photometry;
  in equations, the flux estimate $S_n$ at time $t_n$ of the target star is given by
\begin{eqnarray}
S_n = \frac{1}{M}\,\sum_m w_m\,J_{mn}
\quad ,
\end{eqnarray}
where the sum is over the $M$ pixels that are associated with the target star.
As we have noted above and below, these photometric estimators are not optimal for any purpose,
  but improving them is beyond the scope of this project.

Some---or probably most---of the variations of the pixel intensities
  are being caused by small changes to stellar positions relative to the detector,
  in turn caused by pointing drifts, parallaxes, proper motions, aberration, and temperature changes.
The \name\ pixels, being constructed from a mean prediction plus residuals,
  represent some kind of mean image, averaged over these positional changes.
  that is, the image constructed from the \name\ pixels would have a broader
  point-spread function than any individual time-step,
  and does not necessarily represent any true astronomical scene.
This is another consequence of the point, discussed above,
  that data-driven models do not in general provide interpretable results.
This uninterpretability creates a small problem,
  which is that the optimization by which the \Kepler\ team chose the pixel weights $w_m$
  cannot be precisely appropriate for the \name\ pixels,
  even if it \emph{is} appropriate for the \Kepler\ Target Pixel Files.
That is, there could be a small sacrifice at this step in photometric estimator quality.
This motivates future work on photometric estimation.
That said, we don't expect this effect to be large,
  because the positional drifts involved are orders of magnitude smaller than a pixel.

\todo{DFM: What does the interface look like?  What are the run times and so on?}

\section{Examples and results}

For each example, give the code and show the output and provide some discussion.

We need a metric for comparing outputs.
For now lets try plotting the power spectrum of the SAP, PDC, and \name\ photometry for each star.

So DWH is imagining multi-panel plots: SAP, PDC, and \name\ vs time,
  plus a power-spectrum plot vs frequency.

Cases ought to include
\begin{itemize}
\item a bright, non-variable G star with an Earth injection,
\item a significant variable like that shown in the white paper, and
\item a star with a bad quarter or where \Kepler\ sucked it.
\end{itemize}

Then we want to show some kind of scatter plot, which is some variance estimate from us,
  vs the same estimate from the PDC,
  to show that we always or usually win.
This variance estimate should be something like the power in the residuals at frequencies
  corresponding to 30-hour periods or maybe 50-hour periods or something like that.

\section{Discussion}

Return to the SAP photometry ``unweighted sum'' issue here.

Didn't do anything about short-cadence data; suck it up.

Lots of quantitative and qualitative choices that ought to be explored.
These include what and what and what and what?
How \emph{ought we} make all these choices?

Not the least of these is linearity; deep learning and so on.

The potential for autoregressive models.  Cite the white paper.

The weakest point is construction of photometry from the model plus residuals;
  interesting whether the output of the model can be interrogated
  to provide value for those building physical models.

\acknowledgements
It is a pleasure to thank the whole \Kepler\ Team
  for designing, delivering, and operating a great facility,
  and for making all of the data public, in all its rawest forms, through the MAST interface.
We are also pleased to thank
  Ruth~Angus (Oxford),
  Tom~Barclay (Ames),
  Bekki~Dawson (Berkeley),
  Rob~Fergus (NYU),
  Stefan~Harmeling (Dusseldorf),
  Michael~Hirsch (UCL),
  Dustin~Lang (CMU),
  Benjamin~T.~Montet (Caltech),
  David~Schiminovich (Columbia),
  and
  Jake Vanderplas (UW)
for valuable discussions, input, encouragement, and advice.
This project was partially supported by
  NSF grant IIS-1124794,
  NASA grant NNX12AI50G.
  the Moore Foundation,
  and
  the Sloan Foundation.
This research made use of the NASA Astrophysics Data System.

\todo{White-paper Authors:  Feel free to contribute to this paper and move from the ack list to the author list.}

\clearpage
\begin{figure}
~ % DFM: replace this ~ with a figure
\caption{A patch of \Kepler\ pixels from the Target Pixel Files
  from \todo{XXX quarter}, centered on \todo{YYY target}.
  In the top panel, each box represents a pixel.
  Within each box is shown the light-curve of that pixel over the quarter.
  In the bottom panel, the SAP photometry for this target is shown;
  the SAP photometry is based on a straight sum of the pixels marked with darker borders.
  Note that every pixel varies substantially but the total SAP photometry is better than any individual pixel.
  Note also that the pixel variations are strongly inter-related.\label{fig:pixelpatch}}
\end{figure}

\clearpage
\begin{figure}
~ % DFM: replace this ~ with a figure
\caption{One quarter of photometric data for star \todo{KIC XXX}.
  The top panel shows the \Kepler\ SAP photometry;
    the vertical axis is \todo{something like median-divided brightness}
    and the horizontal axis is \todo{something like KBJD}.
  The next panel shows the \Kepler\ PDC photometry.
  The next shows the \name\ photometry.
  The bottom panel shows the power spectra of the three lightcurves.\label{fig:example1}}
\end{figure}

\end{document}
